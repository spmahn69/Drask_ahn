html

<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>텍스트 문서 보기</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border: 1px solid #ccc;
            overflow: auto;
        }
    </style>
</head>
<body>
    <h1>텍스트 문서 보기</h1>
    <pre id="textContent">
1. 필요한 라이브러리 설치

import pandas as pd
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt

2. Tabular 데이터 로딩

df=pd.read_csv('파일명.csv')  #구분자 sep = ';'
df2=pd.read_csv('파일명.csv', index_col='컬럼3', usecols=['컬럼1','컬럼2','컬럼3']) #인덱스 컬럼 꼭 포함 해야함


3. 데이터 구성 확인

df.head()  #기본 5줄
df.tail()
df.shape()          # row와 col의 개수를 튜플로 반환(row, col 순) >> print(ade.shape)
df.info()             # 데이터 타입, 각 아이템의 개수 등 출력
df.describe()       # count 데이터의 개수 / mean 평균값/ std 표준편차/ min 최소값 / 4분위 수/ max 최대값
df.dtypes()         # 데이터 형태의 종류
df['컬럼명'].value_counts().plot(kind='bar')     # 불균형 확인하는 그래프

4. 상관 분석
4-1. Heatmap(상관관계)

상관관계를 확인하기 위해서 주의해야 할 점은 모든 데이터가 continuous 해야 한다는 점

df.corr()

#컬럼별 상관관계를 heatmap 그래프로 그리기
sns.heatmap(df.corr())
sns.heatmap(df_corr(), annot=True)
plt.show()

# '-' 부호 출력 경고를 제거하기 위한 코드
plt.rc('axes', unicode_minus=False)

5. 데이터 시각화 : Matplotlib
5-1. 선그래프 그리기

# 데이터프레임 : 선 그래프 그리기
df.plot()
df['컬럼명'].plot(figsize=(50, 30))

5-2. 산점도 그리기

# iris.csv의 sepal.width를 x축으로 petal.width를 y축으로 하는 산점도를 시각화
plt.figure(figsize=(16,6))
plt.scatter(x=df['sepal.width'], y=df['petal.width'])
plt,show()

# iris.csv의 sepal.length를 x축으로 petal.lenght를 y축으로 하는 산점도를 시각화
  ( class에 따라 다른색을 띄도록 시각화)
groups=df.groupby('species')
for name, group in groups :
     plt.scatter(x='sepal.length', y='petal.lenght', data=group, label=name)
plt.legend()
plt.show()

# 격자 표시 추가하기
plt.grid()

5-3. 히스토그램 그리기

# iris.csv의 sepal.length 컬럼을 히스토그램으로 시각화 
plt.hist(df['sepal.length'])   # df['sepal.length'].plot(kind='hist')
plt.show()

# iris.csv의 petal.width 컬럼을 히스토그램으로 시각화 
plt.hist(df['petal.width'],bins=5)   
plt.show()

# 컬럼의 분포를 시각화
df['reliability'].value_counts().plot(kind='hist')

5-4. 박스 그래프 그리기

x=[5,3,7,10,9,5,3.5,8]
plt.boxplot(x=x)

df.boxplot(by='컬럼1', column='컬럼2', figsize=(16,8))  # by : 그룹화 컬럼, column : 박스 그래프로 나타낼 컬럼
plt.show()

5-5 막대 그래프 그리기

y=[5,3,7,10,9,5,3.5,8]
x=list(range(len(y)))
plt.figure()
plt.bar(x,y)
plt.show() 

plt.show()
# iris.csv의 species 컬럼을 bar plot을 이용하여 시각화

df[['species']].value_counts().plot(kind ='bar') 
plt.show() 

# species별 sepal.length, petal.length에 대하여 막대 그래프 그리기 
df2=pd.pivot_table(df, index= ['species']) 
df2[['sepal.length', 'petal.length']].plot(kind ='bar') 

# species별 sepal.length, petal.length에 대하여 누적 막대 그래프 그리기
df2[['sepal.length', 'petal.length]].plot(kind='bar', stacked=True) 

# 수평 막대 그래프 그리기 : relative feature importance 시각화 
pit.figure(figsize=(10,6)) 
pit.barh(y=col_names, width=best_tree.feature_importances)
plt.grid() 
plt.show() 

5-6 차트 꾸미기

# 제목과 축 레이블 추가하기 
pit.figure()
plt.plot(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"], [28, 30, 29, 31, 32, 31,31]) 
plt.xlabel('Day') 
plt.ylabel('Temp') 
pit.title('High Temperature') 
plt.show() 

# 한글 폰트 사용하기 
import.matplotlib.font_manager as fm 
fm.findSystemFonts(fontpaths=None, fontext='ttf') 
pit.rc('font', famiIy='Nanum GothicCoding') 
plt.rc('axes', unicode_minus=False) 

# 범례 추가하기 : 2개 선을 가지는 임의의 선그래프를 그리고 범례 추가하기 
pit.plot([l, 2, 3], [1,4,9]) 
pit.plot([2, 3,4], [5,6,7]) 
plt.xlabel('Quarter') 
pit.ylabel('Score') 
pit.title('Game Result') 
plt.legend(['A team', 'B team']) 
plt.show()

# 마커 활용하기 : 3개 선을 각기 다른 색과 마커를 사용해 표현하기  
x=list(range(0, 10)) 
y1=list(range(0,10)) 
y2=list(range(0, 20, 2)) 
y3=list(range( 0,40,4)) 
plt.plot(x, y1, 'r--', x, y2, 'bs', x, y3, 'g^:') 
plt.show() 

# 여러개의 그래프 한번에 그리기 
 
plt.figure(figsize=(20,12)) 

plt.subplot(221)                    # subplot에 넘겨주는 값 = ( 행, 열, 순서) 
plt.plot([1,2,3], [110,130,120]) 
plt.grid() 

plt.subplot(222) 
plt.plot(["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"], [28, 30, 29, 31, 32, 31, 31]) 
plt.xlabel('Day') 
plt.ylabel('Temp') 
plt.title('High Temperature') 
plt.grid() 

plt.subplot(212) 
y=[5, 3, 7, 10, 9, 5, 3.5, 8]
x=list(range(len(y)) 
plt.barh(x, y, height = 0.7, color = "red") 
plt.grid() 
plt.show() 


6. 데이터 시각화 : Seaborn
6-1. Seaborn Install 하기

%pip install seaborn 
%matplotlib inline 
import seaborn as sns 
Import matplotlib.pyplot as plt     # seaborn을 사용할때는 반드시 matplotlib 패키지도 불러와야 함
 

6-2 Scatterplot(산점도) : 두 값 간의 상관 관계  

sns.scatterplot(x='sepal.length', y='sepal.width', data=df) 
plt.show()

 
6-3 Catplot 

sns.catplot(x='sepal.length', y='sepal.width', data=df, col="species", col_wrap=2) 
plt.show() 

6-4 Implot 

Implot은 산점도에 회귀선을 그려주어 하나의 값에 따라 다른 값이 어떻게 변하는 예측할 수 있음

# implot을 사용하여 sepal.length와 sepal.width의 상관관계 확인하기

plt.figure(figsize(10,5)) 
sns.lmplot(x='sepal.length', y='sepal.width', data=df, line_kws={ 'color' : 'red' }) 
plt.show()

# hue 값으로 종에 따라 어떻게 달라지는지 구분하기  
sns.lmplot(x='sepal.length', y='sepal.width', data=df, hue= 'species' ) 
plt.show() 


6-5 Countplot 

항목별 개수를 확인할때 사용

# 나이대별 bill_rating 분포 확인하기
 
plt.figure(figsize=(10,5)) 
sns.countplot( x="by_age, hue="biIl_rating", data = df) 
plt.show() 

히스토그램과 countplot 모두 항목의 개수 분포를 확인하는 그래프이지만
히스토그램은 연속형 변수만 가능하고 countplot은 범주형 변수에도 적용이 가능함

# 연속형 변수 age로 countplot과 비슷한 그래프 그리기  

plt.figure(figsize=(10,5)) 
sns.histplot( x="by_age, bins=20, hue="bill_rating", data = df, multiple = 'dodge', shrink = 0.8) 
plt.show() 

# 성별 고객 등급 분포를 가로 막대 그래프로 확인하기  

sns.countplot( y= "class", hue= "sex", data=df)     # y축 값을 지정하면 가로 막대 그래프로 표현됨 
plt.show() 

# Palette 값을 Spring으로 설정하여 색을 변경하기 

sns.countplot( y= "class", hue= "sex", data=df, palette='spring' ) 
plt.show()


6-6 Jointplot 

산점도와 Countplot을 한번에 보여주어 데이터의 분포와 상관 관계를 한번에 볼 수 있음

sns.jointplot(x="avg_bill", y="age", data=df) 
plt.show()
 
# kind 값을 변경하여 다양한 그래프 그리기 

sns.jointplot( x= "avg_bill", y= "age", data=df, kind ='hex' ) 
plt.showO 


6-7 Boxplot 
describe 함수와 함께 데이터 분포를 확인하는데 유용하게 사용 가능

plt.figure(figsize=(16,8)) 
sns.boxplot(y=df["컬럼1"], x=df["컬럼2"], width=0.9) 

# sns.boxplot(data=df, x="컬럼1', y="컬럼2")
plt.show()


6-8. Violinplot 
Vilonplot은 Boxplot과 모양은 비슷하지만 밀집도를 함께 볼 수 있어 데이터 탐색에
유용하게 사용 가능

# 나이대별 A 상품 요금에 대한 vilonplot 그리기 
plt.figure(figsize=(16,8)) 
sns.violinplot(y=df["A_bill"], x=df["class''], width = 1) 
plt.show() 


6-9 Pairplot 
x축과 y축의 쌍을 지어서 그래프를 그려줌  

sns.pairplot(hue='species', data=df, x_vars=['sepal.length', 'sepal.width' ], y_vars=['입찰가']) 
plt.show()


6-10 Histplot 

sns.histplot(data=df, x='컬럼명1', hue='컬럼별2', hue_order=['0', '1'], bins=13, ked=True) 



7. 데이터전처리-결측치

7-1. 컬럼
 
# 필요한 컬러만 추출하기 
 
df.컬럼명=df['컬럼명']             # Series 형태로 가져온 것 
df.컬럼명=df[['컬럼명']]            # DataFrame 형태로 가져온 것 
df=df[["컬럼1", "컬럼2"]] 

# 컬럼명 바꾸기 
 
df=df.rename(columns = {"컬럼명1_전" : '컬럼명_후', "컬럼명_전" : '컬럼명_후' }) 

# row 선택하기 & Row 와 Column 동시에 선택하기
 
df[7:10]   # 7,8,9 행을 가지고 온다(인덱스 기준)  


# loc 또는 iloc 사용하여 Row 선택하기
 
df.index=np.arrange(100,250)                # df에 인덱스 달기 (시작이 100, 끝이 250)  
df.loc[[100,150,200], ['컬럼1', '컬럼2']]      # 인덱스가 100,150,200 행의 컬럼1, 컬럼2 값을 가져와라  
df.iloc[[0,50,100],[0,1]]                         # 내가 지정한 인덱스가 아니라 원래 인덱스 0,50,100 행의 컬럼 값을 가져와라  

# boolean selection 연산으로 row 선택하기 (=컬럼 조건문으로 행 추출) 
 
extract = df[(df['컬럼1']=='원하는 값') & (df['컬럼2']=='원하는 값')] 


# Column 추가하기 : insert()
 
df['추가 컬럼명'] = df[ '기존 컬럼명'] * 2      # 기존 컬럼의 제일 오른쪽에 새로 추가됨 
df.insert(10, '추가 컬럼명', '기존 컬럼명' *2)   # 10번째 col 순서에 insert  


# ade.csv에 day 컬럼 추가하기 
-date 컬럼을 이용하여 요일을 만드시오  
-요일은 dt.weekday를 활용하여 진행하면 되고, 해당 진행시 integer로 encoding 됨
 
ade['day'] = ade['Date'].dt.weekday          # 월요일이 xx 이다 

# 아래 조건을 만족시키는 wdwe 컬럼을 추가 
-평일(월~금) 이라면 wdwe 컬럼의 값은 1, 평일이면 day의 값이 1,2,3,4
-주말(토~일) 이라면 wdwe 컬럼의 값은 0, 주말이면 day의 값이 5, 6

ade['wdwe'] = 1 
ade.loc[df['day'].isin([5,6]) , 'wdwe'] = 0 


# 컬럼 정의 : rat_r2a 잔류 당분 농도(residual sugar)의 제곱을 알코올 도수로(alcohol)로 나눈 값 
 
wine['rat_r2a'] = wine['residual sugar']**2 / wine['alcohol'] 


컬럼 정의 : rat_cta는 구연산 농도를 (구연산+고정산+휘발산) 농도로 나눈 값으로 정의 

wine['rat_cta'] = wine['citric acid'] / wine[['citric acid', 'volatile acidity', 'fixed acidity']].sum(axis=l) 

# Column 삭제하기 : drop() 
# axis =0 : 행레벨, axis = 1 : 열레벨 
 
df.drop( '삭제할 컬럼', axis = 1)             # 원본에서는 삭제 x  
df1=df.drop( '삭제할 컬럼', axis = 1)       # df1에서는 삭제됨 
df.drop( '삭제할 컬럼', axis = 1, inplace=True)        # 원본에서 삭제됨
df.drop( '삭제할 행의 인덱스', axis = 1, inplace=True) 


# 중복된 열을 찾아 제거하는 코드를 작성 
 
df.drop_duplicates(inplace=True, ignore_index=True) 


# 조건식으로 row 삭제하기 
 
df=df[(df['label']=='0') I (df['label']=='1')] 


# 데이터 타입 변경하기 
 
df=df.astype({'age':int})                             # age 항목을 integer type으로 변경 
df=df.replace('-', np.NaN)                          # '-' 항목을 NaN 값으로 치환 
 
# ade.csv에서 컬럼 Date의 데이터 타입을 날짜 타입으로 변환하는 코드를 작성 
 
ade['Date'] = pd.to_datetime(ade['Date']) 

# 인텍스 초기화 하기  
df=df.reset_index()


7-2 결측치 채우기  

df['컬럼명'].fillna('0', inplace=True)                  # fillna 는 결측치를 찾는 함수 : 결측치를 0으로 바꿔라  
df['컬럼명'].replace(np.nan, ' ' , inplace=True)    # 결측치를 공백으로 바꿔라  
df['컬러명'].replace('_' , 'X', inplace=True)         # '_'를 'X'로 바꾸는 방법  
df=df.fillna(15)                                          # fillna 함수 사용하여 특정 숫자나 문자로 결측치를 처리하는 방법  
# 주의! Method 파라미터 사용시 첫 Record 또는 마지막 Record가 결측치인지 확인해야 함  
df=df.fillna(method='backfill')                       # 뒤에 있는 data를 사용해서 결측치를 처리하는 방법 
df=df.fillna(method='ffill')                            # 앞에 있는 data를 사용해서 결측치를 처리하는 방법  
df['age']=df['age'].replace(np.nan, df['age'].median())     # replace()함수로 결측치 채우기 
df=df.interpolate()                                     # interpolate 함수의 선형 방법을 사용하여 결측값을 채우기  


7-3 결측치 제거하기
  
# 빈칸의 위치를 찾는 코드 
 
print("열 확인 \n", df.isnuIl().any()) 
print("열 확인 \n", df[df.isnuIl().any(axis=1)].index) 

# 과거 4일 사이의 값들의 평균을 계산하고 반올림하여 빈칸 채우는 코드  
# 바로 위 빈칸의 위치를 찾는 코드에서 빈칸이 'Leaflets' 컬럼의 19째줄에 있다는 것을 알려줌 
 
df.loc[19, 'Leaflets'] = np.around(df.loc[15:18, 'Leaflets'].mean()) 


listwise 방식 : record의 항목 중 1개의 값이라도 NA이면 해당 데이터 행 전체 제거 
pairwise 방식 : 모든 항목의 NA인 데이터 행만 제거

df=df.dropna()                            # listwise 방식으로 제거하기  
df=df.dropna(how='all')                 # pairwise 방식으로 제거하기

df=df.dropna(thresh=10)                # 임계치를 설정해서 제거하기 : NA가 아닌 값이 n개 이상인 경우만 남겨 
 
df=df.dropna(subset=['class'])           # 특정 열 안에서만 삭제하기
                                                # 특정 열에 있는 NA만 참고하여 결측치를 제거하려면 Subset 파라미터를 사용하면 됨


# age_cd 컬럼 값중 '_'를 가장 많은 연령코드 값으로 변경 

df_cnt =media_df['age_cd'].value_counts()
media_df.age_cd.replace(["_"],["A07'].inplace = True) 



7-4. 이상치 제거

idx = df[df['선박크기']>350000].index 
df = df.drop(idx) 
idx = df[df['수준잔고']>250].index 
df = df.drop(idx) 


7-5 라벨 인코딩, 원 핫 인코딩

# 라벨 인코딩

from sklearn.preprocessing import LabelEncoder 
Ib = LabelEncoder()

df['species'] = Ib.fitransform(df['species'])              # 범주형 >> 숫자로 
df['species'] = Ib.inverse_transform(df['species'])      # 숫자로 바꾼 것을 범주로 되돌림  


# Object 컬럼에 대해 Pandas get_dummies 함수 사용하여 One Hot Encoding 
 
cal_cols = ['class', 'sex', 'stop', 'npay', 'termination', 'bill_rating'] 
dfl = pd.get_dummies(data=df, columns=cal_cols, drop_first=True)   # drop_first : 첫번째 카테고리 값은 사용하지 않음 


# for문 돌려서 원핫 인코딩 
 
column_list = ['VOC 유형 1 레벨', 'VOC 유형 2 레벨', 'VOC 유형 3 레벨', 'VOC 유형 4 레벨','상품명'] 
for col in column_list : 
df = pd.get_dummies(data=df, columns=[col], drop_first=True) 


7-6 X, Y 데이터 분리

from sklearn.model_selection import train_test_split 
X = df1.drop('termination_Y', axis=1).values                   # x = ade[['Temperature', 'Leaflets', 'price', effect_leaflets']] 
y = df1['termination_Y'].values                                   # y = ade[['Lemon','Orange']] 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size==0.3, stratify=y, random_state=42) 


# Train 데이터를 Train set / Validation set으로 분할
 
from sklearn.modeLselection import train_test_split 
x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=2021) 



7-7 데이터 정규 분포화, 표준화

from sklearn.preprocessing import MinMaxScaler 
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train) 
X_test = scaler.transform(X_test) 
X_train[:2], y_train[:2] 


# wine.csv의 컬럼 density와 PH를 표준화(standardizatlon) 

x = wine[['density', 'pH']] 
wine[['density', 'pH']]= ( x -x.mean(axis=0)) / x.std(axis=0) 

from sklearn.preprocessing import StandardScaler 
scaler=StandardScaler() 

wine[['density', , 'pH']]=scaler.fit_transformer(wine[['density', , 'pH']])



8. 머신러닝 
8-1. 다중회귀 선형분석 모델(회귀)

# 모델학습 

from sklearn.linear_model import LinearRegression 
line_fitter = LinearRegression()
line_fitter.fit(x_train, y_train['Lemon'])       # 트레이닝 셋 (x_train y_train['Lemon'])을 이용하여 학습


# 성능 확인 
 
from sklearn.metrics import mean_absolute_error as MAE 
y_pred=line_fitter.predict(x_test)             # mean absolute error 성능 지표를 통해 확인  
mae = MAE(y_test, y_pred) 


8-2. 로지스틱 회귀 : 종속변수가 범주형이면서 0 또는 1인 경우(분류) 

from sklearn.linear _model import LogisticRegression 
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score 
from sklearn. metrics import classification_report 

Ig = LogisticRegression(C=1.0, max_iter=2000) 
Ig.fit(X_train, y_train) 
Ig.score(X_test, y_test)                # 분류기 성능 평가(score) 
Ig_pred = Ig.predict(X_test)         # 분류기 성능 평가 지표
confusion_matrix(y_test, lg_pred)   # 오차행렬 = Confusion Matrix    # TN FP   # FN TP

accuracy_score(y_test, Ig_pred)      # 정확도 : 상당히 높다  
precision_score(y_test, Ig_pred)      # 정밀도  
recaIl_score(y_test, Ig_pred)           # 재현율 : 상당히 낮다  
f1_score(y_test, Ig_pred)               # 정밀도 + 재현율 

print(classification_report(y_test, Ig_pred))


8-3. KNN (K-Nearest Neighbor) 

from sklearn.neighbors import KNeighborsClassifier 
knn = KNeighborsClassifier(n_neighbors=5) 
knn.fit(X_train, y_train) 
knn_pred = knn.predict(X_test) 


8-4. 결정 트리(DecisionTree) 

from sklearn.tree import DecisionTreeClassifier 
dt = DecisionTreeClassifier(max_depth=10, random_state=42) 
dt.fit(X_train, y _train) 
dt_pred = dt.predict(X_test) 
accuracy_evaI('DecisionTree', dt_pred, y_test) 


# 예제 : Decision Tree 모델 학습(분류) 
- 트레이닝 셋(x_train, y_train)을 이용하여 학습시킨다. 
- 트리의 leaf에는 최소 10개의 샘플이 있다.
- max_depth를 1부터 15까지 늘려가며 총 15개의 트리를 학습시킨다
- 학습시킨 트리들은 리스트를 만들어 trees 변수에 담아둔다
- seed나 random_state는 2021로 고정한다. 


from sklearn.tree import DecisionTreeClassifier 

trees = [ ] 
for depth in range(1,16): 
     tree = DecisionTreeClassifier(max_depth = depth, min_samples_leaf = 10, random_state=2021) 
     tree.fit(x_train, y_train) 
     trees.append(tree) 

# Decision Tree 모델의 성능을 리스트에 담아 accs에 선언하기 바랍니다.
- max_depth가 1인 트리부터 순서대로 평가하여 리스트에 담는다
- 성능 평가는 validation_set를 이용한다
- 성능 지표는 accuracy를 사용한다  

accs = [ ]

for depth in range(1,16) : 
     idx = depth-1 
     tree = trees[idx]
     acc = tree.score(x_valid, y_valid) 
     accs.append(acc) 


# Decision Tree의 max depth에 따른 accuracy를 시각화하고, 가장 성능이 좋은 depth를 선택
- 동일성능의 depth가 여러개라면 가장 적은 depth를 선택한다

plt.figure(figsize=(10,6)) 
plt.plot(range(1,16), accs) 
plt.grid()
plt.show() 
print('depth = 3')    # depth =3 선택
best_tree = trees[2] 


# relative feature importance 시각화
 
plt.figure(figsize=(10,6)) 
plt. barh(y=col_names, width=best_tree. feature_importances_ ) 
plt.grid()
plt.show() 

# accuracy 출력
 
acc=best_tree.score(x_valid, y_valid) 



9. 앙상블 기법의 종류
9-1. 랜덤 포레스트(RandomForest)

# 주요 Hyperparameter 
-random_state: 랜덤 시드 고정 값. 고정해두고 튜닝할 것! 
-n_jobs: CPU 사용 갯수 
-max_depth: 깊어질수 있는 최대 깊이. 과적합 방지용 
-n_estimators: 앙상블하는 트리의 갟수 
-max_features: 최대로 사용할 feature의 갯 수. 과적합 방지용  
-min_samples_split: 트리가 분할할 때 최소 샘플의 갯 수. default=2 과적합 방지용 


# 모델링

from sklearn.ensemble import RandomForestClassifier 

rfc = RandomForestCiassifier(n_estimators=3, max_features=9, max_depth=13, random_state=2021, min_samples_leaf=5) 
rfc.fit(x_train, y_train) 

# 모델 성능 평가 
 
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score 

rfc.score(x_test, y_test) 
rfc_pred = rfc.predict(x_test) 


# 예제 : 랜덤 포레스트 성능 평가
- y값을 예측하여 confusion matrix를 구하고 heat map 그래프를 시각화 하세요 
- Scikit-Iearn 의 classification report 기능을 사용하여 성능을 출력하세요 

from sklearn.metrics import classification_report 

cm = confusion_matrix(y_test, rfc_pred) 
sns.heatmap(cm, annot=True)                # annotation : 주석 
print(classification_report(y_test, rfc_pred, target_names=['class 0', 'class 1'])) 


# 예제 : 랜덤 포레스트 모델들을 학습시키기 바랍니다. 
-RandomForestClasslfier 하이퍼파라미터 설정 : n_estimators=나무의 개수, max_depth=13(각 트리의 max depth)
 min_samples_leaf=5(한 개의 node에 최소한의 데이터 개수, 5개면 tree depth를 늘리지 않음) 
- 와인 퀄러티를 '분류' 모델링 한다
- 트레이닝 셋(x_train, y_train)을 이용하여 학습시킨다
- 나무 개수를 1에서 50까지 늘려가며 학습시킨다
- 학습시킨 랜덤 포레스트들은 리스트를 만들어 forests 변수에 담아둔다
- seed나 random_state는 2021로 고정한다

from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import accuracy_score 

forests = [ ] 
forests.append(forest) 


# 예제 1 : RandomForest 모델들의 성능을 리스트에 담아 accs에 선언하시기 바랍니다.
- 웨에 저장한 randomforest 값들을 불러온다(forest list를 활용한다. forest[i] 형태)
- for문 함수를 사용하여 각 모델에 대한 valid set에 대한 acc를 확인한다(score 함수 활용)
- validation set 위에서 성능을 평가한다
- append 를 통해 accs list의 값에 계속 해당 성능 값을 추가한다

aces = [ ]

for i in range(50) : 
     forest = forests[i] 
     ace = forest.score(x_valid, y_valid) 
     accs.append(acc) 


# RandomForest의 트리 개수에 따른 accuracy를 시각화하기 바랍니다.
- 위에서 제작한 accs를 이용한다
- line plot을 이용하여 각 모델 별 accuracy를 출력한다
- 동일 성능의 gamma 가 여러개라면 가장 작은걸 선택한다

plt.figure(figsize=(10, 6)) 
plt.plot([0.1+i for i in range(50)], accs) 
plt.grid() 
plt.show() 


# 예제 2 : 모델의 성능을 평가
- 예측결과의 confusion matrix를 구하고 heat map 그래프 및 클래스별 성능을 출력
- 최초 예측 확률 값 저장(y_prob), 예측 결정 내용(y_pred)은 괄호 안의 변수를 사용하고, 예측 값의 기준은 0.5로 구분
- Heatmap 그래프에 annotation을 포함시켜 confusion matrix를 그리세요
- Scikit-Iearn 의 classification report 기능을 사용하여 class별 precision, recall, f1-score를 출력하세요 

from sklearn.metrics import confusion_matrix 
from sklearn.metrics import classification_report 

y_prob = rfc.predict_proba(X_valid)              # 최초 예측 확률 값 
y_pred = [1 if x > 0.5 else 0 for x in y_prob] 
cm = confusion_matrix(y_valid. y_pred) 
sns.heatmap(cm, annot=True) 
print(classification_report(y_valid, y_pred)) 


# RandomForestRegression (회귀)
 
from sklearn.ensemble import RandomForestRegressor 

rfr = RandomForestRegressor(n_estimators=70, max_depth=12. random_state=42) 
model == rfr.fit(x_train.y_train) 

from sklearn.metrics import mean_squared_error as MSE 
from sklearn.metrics import mean_absolute_error as MAE 

y_pred = model.predict(x_valid) 
print(f"MAE of Randomforest : {MAE(y_valid, y_pred)**.5:, 2f}") 
print(f"MSE of Randomforest : {MSE(y_valid, y_pred)**.5:, 2f}") 

 
9-2. XGBoost 

**주요 파라미터**
-random_state: 랜담 시드 고정 값. 고정해두고 튜닝할 것! 
-n_jobs: CPU 사용 개수 
-learning_rate: 학습율. 너무 큰 학습율은 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 저적한 값을 찾아야 함
                   n_estimators와 같이 튜닝. default=0.1 
-n_estimators: 부스팅. 스테이지 수.(랜덤 포레스트 트리의 개수 설정과 비슷한 개념). default=100 
-max_depth: 트리의 깊이. 과적합 방지용. default=3. 
-subsample: 샘플 사용 비율. 과적합 방지용 default=1.0 
-max_features: 최대로 사용할 feature의 비율. 과적합 방지용. default=1.0 

!pip install xgboost 
from xgboost import XGBClassifier 
xgb_pred = xgb.predict(X_test) 
accuracy_evaI('XGBoost', xgb_pred, y-test) 


9-3. Light GBM 

**주요 파라미터**
-random_state: 랜담 시드 고정 값. 고정해두고 튜닝할 것! 
-n_jobs: CPU 사용 개수 
-learning_rate: 학습율. 너무 큰 학습율은 성능을 떨어뜨리고, 너무 작은 학습율은 학습이 느리다. 저적한 값을 찾아야 함
                   n_estimators와 같이 튜닝. default=0.1 
-n_estimators: 부스팅. 스테이지 수.(랜덤 포레스트 트리의 개수 설정과 비슷한 개념). default=100 
-max_depth: 트리의 깊이. 과적합 방지용. default=3. 
-colsample_bytree : 샘플 사용 비율(max_features와 비슷한 개념). 과적합 방지용. default=1.0 

!pip install lightgbm 
from lightgbm import LGBMClassifier 
Igbm = LGBMClassifier(n_estimators=3, random_state=42)    # 1분 소요 
Igbm.fit(X_train, y_train) 
Igbm_pred = Igbm.predict(X_test) 
accuracy_evaI('LGBM', Igbm_pred, y_test) 


9-4. Stacking 

from sklearn.ensemble import StackingRegressor, StackingClassifier 

stack_models = [ 
                     ('Logistic Regression', Ig), 
                     ('KNN', knn), 
                     ('DecisionTree', dt), 
]


# stack models로 선언된 모델(LogisticRegression,KNN,DecisionTree)의 예측 결과를 최종
meta_model(final_estimator)을 RandomForest(rfc) 사용하여 분류 예측 
stacking = StackingClassifier(stack_models, final_estimator=rfc, njobs=-1) 
stacking.fit(X_train, y-train)    # 1분 20초 소요 

stacking_pred = stacking.predict(X_test) 
accuracy_eval('Stacking Ensemble', stacking_pred, y_test) 


9-5. Weighted Blending 

final_outputs = { 
                  'DecisionTree': dt_pred, 
                  'randomforest': rfc_pred, 
                  'xgb': xgb_pred, 
                  'Igbm': Igbm_pred, 
                  'stacking': stacking_pred, 
} 

final_pred iction=\ 
finaLoutputs['DecisionTree'] *0.1\ 
+final_outputs['randomforest']*0.2\ 
+final_outputs['xgb'] * 0.25\ 
+final_outputs['lgbm'] * 0.15\ 
+final_outputs['stacking'] * 0.3\ 

# 가중치 계산값이 0.5 초과하면 1, 그렇지 않으면 0  
finaLprediction = np.where(final_prediction > 0.5, 1, 0) 

accuracy_eval('Weighted Blending', final_prediction, y_test) 


10. 딥러닝 모델링
10-1. 뉴럴 네트워크(분류)

import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras.layers import Input, Dense, BatchNormalization 
from tensorflow.keras.models import Sequential, Model 
from tensorflow.keras.callbacks import EarlyStopping 

keras.backend.clear_session() 

model = Sequential() 
model.add(Dense(36, activation='relu', input_shape=(6,)))       # 입력변수의 수 = feature의 개수 
model.add(Dropout(0.3))                                               # 과적합 방지하는 Dropout 
model.add(Dense(36, activation='relu'))                             # 36-36 하면 resource를 많이 사용 
model.add(Dropout(0.3))                                               # 만약 BatchNormalization 사용하라고 하면 model add(BatchNormalization()) 
model.add(Dense(36, activation='relu'))                             # 36, 64 숫자는 중요하지 않음 
model.add(Dropout(0.3)) 
model.add(Dense(1. activation='softmax'))                          # 결과값(y값, 라벨)이 원핫인토딩된 상태일 수도 있다 

                                              activation                         loss
이진분류                                sigmoid(dense=1)         binary_crossentropy
다중분류(원핫인코딩 o)                  softmax                categorical_crossentropy
다중분류(원핫인코딩 x)                  softmax                sparse_categorical_crossentropy
회귀                                          relu                    MSE(Mean Square Error), MAE(Mean Absolute Error), RMES
                                                              

model.compile(optimizer='adam', loss='categoricaI_crossentropy', metrics= ['accuracy']) 

es = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=l, 
restore_best_weights=True, mode='min')                        # mode= min or max 
history = model.fit(x_train, y_train, epochs=2000, batch_size=32, 
verbose=1, validation_data=(x_test, y_test), callbacks=[es])   # patience :Validation loss가 100번 이상 개선되지 않으면 학습 중단 

# 학습 로그 시각화 
import matplotlib.pyplot as plt 

plt.plot(history history['acc racy']) 
plt.plot(history.history['loss'], 'blue', label='loss') 
plt.plot(history.history['val_accuracy']) 
plt.plot(history.history['val_loss'], 'black', label='val_loss') 
plt.title('Accuracy') 
plt.xlabeI('epochs') 
plt.ylabeI('accuracy') 
plt.legend()             
plt.show()


10-2. 뉴럴 네트워크(회귀)  

import tensorflow as tf 
from tensorflow import keras 
from tensorflow.keras.layers import Input, Dense, BatchNormalization 
from tensorflow.keras.models import Sequential, Model 
from tensorflow.keras.callbacks import EarlyStopping 

keras. backend.clear_session() 

model = Sequential() 
model.add(Dense(36, activation ='relu', input_shape=(6,))) 
model.add(Dropout(0.3)) 
model.add(Dense(36, activation='relu')) 
model.add(Dropout(0.3)) 
model.add(Dense(36, activation='relu')) 
model.add(Dropout(0..3) ) 
model.add(Dense(1, activation='relu')) 

# 학습 로그 시각화
 
import matplotlib.pyplot as plt 

plt.plot(history. history['loss']) 
plt.plot(history.history['val_loss']) 
pIt.title('Loss') 
plt.xlabel('epochs') 
plt.ylabeI('MSE') 
plt.legend(['train_loss', 'val_loss']) 
plt.show()


# 뉴럴네트워크 예측 결과의 loss값 출력 

from sklearn.metrics import mean_squared_error as MSE 

y _pred= model. predict(x_test) 
mse = MSE(y_test, y_pred) 
print(mse) 


# Tensorflow framework를 사용하여 딥러닝 모델을 만드세요 
-히든레이어(hidden layer) 3개 이상으로 모델을 구성하고 과적합을 방지하는 dropout 을 설정 
-EarlyStopping 콜백으로 정해진 epoch 동안 모니터링 지표가 향상되지 않을 때 훈련을 중지하도록 설정  
-ModelCheckpoint 콜백으로 validation performance가 좋은 모델을 핸드폰번호.h5 파일로 저장 

from tensorflow.keras.models import Sequential, load_model 
from tensorflow.keras.layers import Dense, Dropout 
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint 
from tensorflow.keras.utils import to_categorical 

es = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, restore_best_weights=True, mode='min')  # mode = 'max' 
mc=ModelCheckpoint('핸드폰번호.h5', monitor='val_loss', save_weight_only= True, save_best_only= True, verbose=1) 

model=Sequential() 
model.add(Dense(128, activation='relu', input_shape=(18,))) 
model.add(Dropout(0.2)) 

model.add(Dense(64,activation='relu')) 
model.add(Dropout(0.2)) 

model.add(Dense(32,activation='relu')) 
model.add(Dropout(0.2)) 

model.add (Dense(16, activat ion ='relu')) 
model.add(Dropout(0.2)) 

model.add(Dense(1,activation='sigmoid')) 
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])    # metrics가 acc이면 
# sigmoid 일때 binary_crossentropy도 한다 >>이진 분류일때  
# 이진분류 이상일때 category  


y_train_ohe = to_categorical(y_train) 
y_test_Cohe = to_categorical(y_test) 
history = model.fit(x_train, y_train, batch size=1024, epochs=20, callbacks=[es,mc],  
validation_data=(x_test, y _test_ohe), verbose=1) 


10-3. DNN 

A. 이진 분류용 DNN layer 

import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Activation, Dropout 

# 187개 input layer 
# unit 4개 hidden layer 
# unit 3개 hidden layer 
# 1개 output layer : 이진 분류 


model = Sequential()
model.add(Dense(4, activation='relu', input_shape=(18,))) 
model.add(Dense(3, activation='relu')) 
model.add (Dense(1, activation ='sigmoid')) 

model.summary() 


# 모델 컴파일 - 이진 분류 모델

model. compile(optimizer='adam', loss='binary_crossentropy', metrics= ['accuracy']) 

model = Sequential()
model.add(Dense(4, activation='relu', input_shape=(18,))) 
model.add(Dropout(0.3)) 
model.add(Dense(3, activation='relu')) 
model.add(Dropout(0.3)) 
model.add(Dense(1, activation='sigmoid')) 

model.summary() 


# 모델 훈련(학습) 하기 

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=16) 


B. 다중 분류용 DNN layer 

# 18개 input layer 
# unit 5개 hidden layer 
# dropout 
# unit 4개 hidden layer 
# dropout 
# 2개 output layer : 다중분류 

model = Sequential() 
model.add(Dense(5, activation='relu', input_shape=(18,)))
model.add(Dropout(0.3)) 
model.add(Dense(4, activation='relu')) 
model.add(Dropout(0.3)) 
model.add(Dense(2, activation='softmax')) 

model.summary()


# 모델 컴파일 - 다중 분류 모델 

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics= ['accuracy']) 

# 모델 훈련(학습) 하기  

history = model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=20, batch_size=16) 


# 성능 시각화  

losses = pd. DataFrame(model.history.history) 
losses.head() 
losses[['loss', 'val_loss']].plot() 
losses[['loss', 'val_loss', 'accuracy', 'val_accuracy']].plot() 
plt.plot(history.history['accuracy']) 
plt.plot(history.history['val_accuracy']) 
plt.title('Accuracy') 
plt.xlabel('Epochs') 
plt.ylabeI('Acc') 
plt.legend(['acc', 'val_acc']) 
plt.show() 

10-4. RNN 

# RNN 라이브러리 import
 
import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Flatten 
from tensorflow.keras.layers import LSTM 

X_train.shape, X_test.shape


X_train = X_train.reshape(-1,18,1) 
X_test = X_test.reshape(-1,18,1) 
X_train.shape, X_test.shape 


# define model 
model = Sequential() 
model.add(LSTM(32, activation='relu', return_sequences=True, input_shape=(18, 1))) 
model.add(LSTM (16, activation=' relu', return_sequences= True)) 
model.add(Flatten()) 
model.add(Dense(8, activation='relu')) 
model.add(Dense(1, activation ='sigmoid')) 

model.summary()


# 모델 컴파일-이진 분류 모델 

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) 


# 모델 학습 

history = model.fit(x=X_train, y=y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), verbose=1) 


# 성능 시각화 - 성능 평가 

losses = pd.DataFrame(model.history.history) 
losses.head()
losses[['loss', 'val_loss']].plot() 
losses[['loss', 'val_loss', 'accuracy', 'val_accuracy']].plot() 
plt.plot(history.history['accuracy']) 
plt.plot(history.history['val_accuracy']) 
plt.title('Accu racy') 
plt.xlabel('Epochs') 
plt.ylabeI('Acc') 
plt.legend(['acc', 'val_acc']) 
plt.show() 


    </pre>
</body>
</html>













